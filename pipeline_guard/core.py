#!/usr/bin/env python3
"""
Self-Healing Pipeline Guard Core System
Central orchestration and control for pipeline health management

Features:
- Real-time pipeline monitoring and health assessment
- Intelligent anomaly detection using HDC pattern recognition
- Automated repair and recovery mechanisms
- Predictive failure analysis and prevention
- Enterprise-grade security and audit logging
"""

import time
import json
import asyncio
import threading
import logging
from enum import Enum
from typing import Dict, List, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
import uuid

# HDC imports for pattern recognition
from hdc_robot_controller.core.hypervector import HyperVector
from hdc_robot_controller.core.memory import HierarchicalMemory
from hdc_robot_controller.core.behavior_learner import BehaviorLearner
from hdc_robot_controller.core.logging_system import setup_production_logging
from hdc_robot_controller.robustness.advanced_error_recovery import AdvancedErrorRecovery

class GuardStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded" 
    CRITICAL = "critical"
    HEALING = "healing"
    FAILED = "failed"

class PipelinePhase(Enum):
    BUILD = "build"
    TEST = "test"
    DEPLOY = "deploy"
    MONITOR = "monitor"
    ROLLBACK = "rollback"

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    EMERGENCY = "emergency"

@dataclass
class PipelineMetrics:
    """Comprehensive pipeline health metrics"""
    timestamp: datetime
    pipeline_id: str
    phase: PipelinePhase
    
    # Performance metrics
    duration_seconds: float
    success_rate: float
    error_count: int
    warning_count: int
    
    # Resource metrics
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    network_io: float
    
    # Quality metrics
    test_coverage: float
    code_quality_score: float
    security_score: float
    
    # Pipeline-specific metrics
    build_time: float = 0.0
    test_time: float = 0.0
    deploy_time: float = 0.0
    queue_time: float = 0.0
    
    # HDC-encoded pattern signature
    pattern_signature: Optional[HyperVector] = None
    
    def get_health_score(self) -> float:
        """Calculate overall health score (0.0-1.0)"""
        weights = {
            'success_rate': 0.3,
            'performance': 0.2,
            'resources': 0.2,
            'quality': 0.2,
            'stability': 0.1
        }
        
        # Performance score (inverse of duration relative to baseline)
        baseline_duration = 300.0  # 5 minutes baseline
        performance_score = min(1.0, baseline_duration / max(1.0, self.duration_seconds))
        
        # Resource score (inverse utilization)
        resource_score = 1.0 - max(self.cpu_usage, self.memory_usage, self.disk_usage)
        
        # Quality score (average of quality metrics)
        quality_score = (self.test_coverage + self.code_quality_score + self.security_score) / 3.0
        
        # Stability score (inverse error rate)
        stability_score = max(0.0, 1.0 - (self.error_count / max(1.0, self.error_count + 100)))
        
        total_score = (
            weights['success_rate'] * self.success_rate +
            weights['performance'] * performance_score +
            weights['resources'] * resource_score +
            weights['quality'] * quality_score +
            weights['stability'] * stability_score
        )
        
        return max(0.0, min(1.0, total_score))

@dataclass
class GuardAlert:
    """Alert generated by pipeline guard"""
    id: str
    timestamp: datetime
    severity: AlertSeverity
    pipeline_id: str
    phase: PipelinePhase
    title: str
    message: str
    metrics: Dict[str, Any]
    recommended_actions: List[str] = field(default_factory=list)
    auto_repair_attempted: bool = False
    resolved: bool = False
    resolution_time: Optional[datetime] = None

class PipelineGuard:
    """
    Main self-healing pipeline guard system
    
    Orchestrates monitoring, detection, and repair of CI/CD pipelines using
    advanced HDC-based pattern recognition and machine learning.
    """
    
    def __init__(self, 
                 config_path: Optional[str] = None,
                 hdc_dimension: int = 10000,
                 enable_auto_repair: bool = True,
                 enable_predictive_analysis: bool = True):
        
        self.hdc_dimension = hdc_dimension
        self.enable_auto_repair = enable_auto_repair
        self.enable_predictive_analysis = enable_predictive_analysis
        
        # Initialize logging
        self.logger = setup_production_logging(
            "/app/logs/pipeline_guard.log",
            "INFO",
            True
        )
        
        # Core HDC components for pattern recognition
        self.memory = HierarchicalMemory(hdc_dimension)
        self.behavior_learner = BehaviorLearner(hdc_dimension)
        self.error_recovery = AdvancedErrorRecovery(hdc_dimension)
        
        # Guard state
        self.status = GuardStatus.HEALTHY
        self.active_pipelines: Dict[str, Dict[str, Any]] = {}
        self.alert_history: List[GuardAlert] = []
        self.metrics_history: List[PipelineMetrics] = []
        
        # Monitoring and analysis components
        self.monitors: Dict[str, Any] = {}
        self.detectors: Dict[str, Any] = {}
        self.repair_engines: Dict[str, Any] = {}
        
        # Real-time processing
        self.monitoring_active = False
        self.monitoring_thread: Optional[threading.Thread] = None
        self.event_queue: asyncio.Queue = asyncio.Queue()
        
        # Pattern learning for failure prediction
        self.failure_patterns = {}
        self.success_patterns = {}
        self.performance_baselines = {}
        
        # Configuration
        self.config = self._load_configuration(config_path)
        
        self.logger.info("Pipeline Guard initialized", 
                        hdc_dimension=hdc_dimension,
                        auto_repair=enable_auto_repair,
                        predictive_analysis=enable_predictive_analysis)
    
    def _load_configuration(self, config_path: Optional[str]) -> Dict[str, Any]:
        """Load pipeline guard configuration"""
        
        default_config = {
            "monitoring": {
                "interval_seconds": 30,
                "metrics_retention_days": 30,
                "alert_retention_days": 90
            },
            "thresholds": {
                "health_score_warning": 0.7,
                "health_score_critical": 0.5,
                "error_rate_warning": 0.05,
                "error_rate_critical": 0.15,
                "performance_degradation_threshold": 0.3
            },
            "auto_repair": {
                "max_attempts": 3,
                "cooldown_minutes": 10,
                "escalation_enabled": True
            },
            "notifications": {
                "webhook_url": None,
                "email_enabled": False,
                "slack_enabled": False
            }
        }
        
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r') as f:
                    user_config = json.load(f)
                    # Merge with defaults
                    default_config.update(user_config)
            except Exception as e:
                self.logger.error(f"Failed to load config from {config_path}: {e}")
        
        return default_config
    
    def start_monitoring(self):
        """Start continuous pipeline monitoring"""
        
        if self.monitoring_active:
            self.logger.warning("Monitoring already active")
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitoring_thread.start()
        
        self.logger.info("Pipeline monitoring started")
    
    def stop_monitoring(self):
        """Stop pipeline monitoring"""
        
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=10)
        
        self.logger.info("Pipeline monitoring stopped")
    
    def _monitoring_loop(self):
        """Main monitoring loop"""
        
        self.logger.info("Pipeline monitoring loop started")
        
        while self.monitoring_active:
            try:
                # Collect metrics from all active pipelines
                current_metrics = self._collect_pipeline_metrics()
                
                # Process each pipeline's metrics
                for metrics in current_metrics:
                    self._process_pipeline_metrics(metrics)
                
                # Update overall guard status
                self._update_guard_status()
                
                # Perform predictive analysis
                if self.enable_predictive_analysis:
                    self._perform_predictive_analysis()
                
                # Clean up old data
                self._cleanup_old_data()
                
                # Wait for next monitoring cycle
                time.sleep(self.config["monitoring"]["interval_seconds"])
                
            except Exception as e:
                self.logger.error(f"Error in monitoring loop: {e}")
                time.sleep(10)  # Brief pause before retry
    
    def _collect_pipeline_metrics(self) -> List[PipelineMetrics]:
        """Collect metrics from all monitored pipelines"""
        
        metrics_list = []
        
        # In real implementation, this would integrate with CI/CD systems
        # (Jenkins, GitLab CI, GitHub Actions, etc.)
        
        # Simulate metrics collection for demo
        for pipeline_id, pipeline_info in self.active_pipelines.items():
            try:
                metrics = self._simulate_pipeline_metrics(pipeline_id, pipeline_info)
                metrics_list.append(metrics)
                
            except Exception as e:
                self.logger.error(f"Failed to collect metrics for pipeline {pipeline_id}: {e}")
        
        return metrics_list
    
    def _simulate_pipeline_metrics(self, pipeline_id: str, pipeline_info: Dict[str, Any]) -> PipelineMetrics:
        """Simulate realistic pipeline metrics for demonstration"""
        
        import random
        import numpy as np
        
        # Base metrics with realistic variation
        base_duration = pipeline_info.get('expected_duration', 600)  # 10 minutes
        base_success_rate = pipeline_info.get('base_success_rate', 0.95)
        
        # Add realistic variation and occasional issues
        duration_variation = np.random.normal(1.0, 0.2)  # ±20% variation
        duration = max(60, base_duration * duration_variation)
        
        # Simulate occasional issues
        issue_probability = 0.1  # 10% chance of issues
        if random.random() < issue_probability:
            duration *= 1.5  # Issues increase duration
            success_rate = max(0.5, base_success_rate * 0.8)
            error_count = random.randint(1, 5)
        else:
            success_rate = min(1.0, base_success_rate + random.uniform(-0.05, 0.05))
            error_count = 0
        
        # Resource usage simulation
        cpu_usage = min(0.95, 0.3 + random.random() * 0.4)
        memory_usage = min(0.90, 0.4 + random.random() * 0.3)
        disk_usage = min(0.85, 0.2 + random.random() * 0.3)
        network_io = random.random() * 0.8
        
        # Quality metrics
        test_coverage = min(1.0, 0.7 + random.random() * 0.25)
        code_quality = min(1.0, 0.8 + random.random() * 0.15)
        security_score = min(1.0, 0.85 + random.random() * 0.10)
        
        # Phase-specific timing
        build_time = duration * 0.4
        test_time = duration * 0.3  
        deploy_time = duration * 0.2
        queue_time = duration * 0.1
        
        metrics = PipelineMetrics(
            timestamp=datetime.now(),
            pipeline_id=pipeline_id,
            phase=random.choice(list(PipelinePhase)),
            duration_seconds=duration,
            success_rate=success_rate,
            error_count=error_count,
            warning_count=random.randint(0, 3),
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            disk_usage=disk_usage,
            network_io=network_io,
            test_coverage=test_coverage,
            code_quality_score=code_quality,
            security_score=security_score,
            build_time=build_time,
            test_time=test_time,
            deploy_time=deploy_time,
            queue_time=queue_time
        )
        
        # Generate HDC pattern signature for advanced analysis
        metrics.pattern_signature = self._generate_pattern_signature(metrics)
        
        return metrics
    
    def _generate_pattern_signature(self, metrics: PipelineMetrics) -> HyperVector:
        """Generate HDC pattern signature for pipeline metrics"""
        
        # Create hypervector encoding of pipeline state
        # This enables pattern recognition and anomaly detection
        
        # Encode different aspects of pipeline health
        performance_hv = HyperVector.random(self.hdc_dimension)
        performance_hv = performance_hv.scale(metrics.duration_seconds / 600.0)  # Normalize to 10min baseline
        
        quality_hv = HyperVector.random(self.hdc_dimension)
        quality_hv = quality_hv.scale(metrics.get_health_score())
        
        resources_hv = HyperVector.random(self.hdc_dimension) 
        resource_level = (metrics.cpu_usage + metrics.memory_usage + metrics.disk_usage) / 3.0
        resources_hv = resources_hv.scale(resource_level)
        
        errors_hv = HyperVector.random(self.hdc_dimension)
        error_level = min(1.0, metrics.error_count / 10.0)  # Normalize error count
        errors_hv = errors_hv.scale(error_level)
        
        # Combine into composite pattern signature
        pattern_signature = performance_hv.bundle([quality_hv, resources_hv, errors_hv])
        
        return pattern_signature
    
    def _process_pipeline_metrics(self, metrics: PipelineMetrics):
        """Process pipeline metrics and generate alerts if needed"""
        
        # Store metrics in history
        self.metrics_history.append(metrics)
        
        # Update pipeline state
        self.active_pipelines[metrics.pipeline_id]['last_metrics'] = metrics
        self.active_pipelines[metrics.pipeline_id]['last_update'] = datetime.now()
        
        # Evaluate health and generate alerts
        health_score = metrics.get_health_score()
        
        # Check for threshold violations
        alerts = []
        
        if health_score < self.config["thresholds"]["health_score_critical"]:
            alerts.append(self._create_alert(
                AlertSeverity.CRITICAL,
                metrics,
                "Critical Health Score",
                f"Pipeline health score {health_score:.2f} below critical threshold"
            ))
        elif health_score < self.config["thresholds"]["health_score_warning"]:
            alerts.append(self._create_alert(
                AlertSeverity.WARNING,
                metrics,
                "Low Health Score", 
                f"Pipeline health score {health_score:.2f} below warning threshold"
            ))
        
        # Check error rate
        if metrics.error_count > 0:
            error_rate = metrics.error_count / max(1, metrics.error_count + 50)  # Assume ~50 total operations
            
            if error_rate > self.config["thresholds"]["error_rate_critical"]:
                alerts.append(self._create_alert(
                    AlertSeverity.CRITICAL,
                    metrics,
                    "High Error Rate",
                    f"Error rate {error_rate:.1%} exceeds critical threshold"
                ))
            elif error_rate > self.config["thresholds"]["error_rate_warning"]:
                alerts.append(self._create_alert(
                    AlertSeverity.WARNING,
                    metrics,
                    "Elevated Error Rate",
                    f"Error rate {error_rate:.1%} exceeds warning threshold"
                ))
        
        # Performance degradation check
        baseline_duration = self.performance_baselines.get(metrics.pipeline_id, 600)
        if metrics.duration_seconds > baseline_duration * (1 + self.config["thresholds"]["performance_degradation_threshold"]):
            alerts.append(self._create_alert(
                AlertSeverity.WARNING,
                metrics,
                "Performance Degradation",
                f"Pipeline duration {metrics.duration_seconds:.0f}s exceeds baseline by {((metrics.duration_seconds / baseline_duration) - 1) * 100:.0f}%"
            ))
        
        # Process alerts
        for alert in alerts:
            self._handle_alert(alert)
        
        # Learn from patterns
        self._learn_from_metrics(metrics)
    
    def _create_alert(self, severity: AlertSeverity, metrics: PipelineMetrics, 
                     title: str, message: str) -> GuardAlert:
        """Create a new alert"""
        
        recommended_actions = self._generate_recommended_actions(severity, metrics)
        
        alert = GuardAlert(
            id=str(uuid.uuid4()),
            timestamp=datetime.now(),
            severity=severity,
            pipeline_id=metrics.pipeline_id,
            phase=metrics.phase,
            title=title,
            message=message,
            metrics={
                'health_score': metrics.get_health_score(),
                'duration': metrics.duration_seconds,
                'error_count': metrics.error_count,
                'cpu_usage': metrics.cpu_usage,
                'memory_usage': metrics.memory_usage
            },
            recommended_actions=recommended_actions
        )
        
        return alert
    
    def _generate_recommended_actions(self, severity: AlertSeverity, 
                                    metrics: PipelineMetrics) -> List[str]:
        """Generate recommended actions for addressing issues"""
        
        actions = []
        
        # Performance-based recommendations
        if metrics.duration_seconds > 900:  # > 15 minutes
            actions.append("Consider optimizing build process or increasing resources")
        
        # Resource-based recommendations
        if metrics.cpu_usage > 0.9:
            actions.append("Scale up CPU resources or optimize CPU-intensive tasks")
        
        if metrics.memory_usage > 0.85:
            actions.append("Increase memory allocation or optimize memory usage")
        
        # Quality-based recommendations
        if metrics.test_coverage < 0.7:
            actions.append("Improve test coverage to reduce failure risk")
        
        if metrics.code_quality_score < 0.8:
            actions.append("Address code quality issues identified by analysis tools")
        
        # Error-based recommendations
        if metrics.error_count > 0:
            actions.append("Review error logs and fix failing tests or build issues")
        
        # Severity-specific actions
        if severity == AlertSeverity.CRITICAL:
            actions.append("Consider rolling back to last known good state")
            actions.append("Escalate to on-call engineer immediately")
        
        return actions
    
    def _handle_alert(self, alert: GuardAlert):
        """Handle generated alert"""
        
        # Store alert
        self.alert_history.append(alert)
        
        # Log alert
        self.logger.warning(f"Pipeline Alert: {alert.title}",
                           pipeline_id=alert.pipeline_id,
                           severity=alert.severity.value,
                           message=alert.message)
        
        # Attempt auto-repair if enabled and appropriate
        if (self.enable_auto_repair and 
            alert.severity in [AlertSeverity.WARNING, AlertSeverity.CRITICAL]):
            self._attempt_auto_repair(alert)
        
        # Send notifications
        self._send_notifications(alert)
    
    def _attempt_auto_repair(self, alert: GuardAlert):
        """Attempt automated repair for the alert"""
        
        self.logger.info(f"Attempting auto-repair for alert {alert.id}")
        
        # Mark auto-repair attempted
        alert.auto_repair_attempted = True
        
        # Determine repair strategy based on alert type and metrics
        repair_strategies = self._determine_repair_strategies(alert)
        
        for strategy in repair_strategies:
            try:
                success = self._execute_repair_strategy(alert, strategy)
                if success:
                    alert.resolved = True
                    alert.resolution_time = datetime.now()
                    self.logger.info(f"Auto-repair successful for alert {alert.id} using strategy: {strategy}")
                    break
            except Exception as e:
                self.logger.error(f"Auto-repair strategy {strategy} failed for alert {alert.id}: {e}")
        
        if not alert.resolved:
            self.logger.warning(f"All auto-repair attempts failed for alert {alert.id}")
    
    def _determine_repair_strategies(self, alert: GuardAlert) -> List[str]:
        """Determine appropriate repair strategies for an alert"""
        
        strategies = []
        
        # Strategy selection based on alert patterns
        if "performance" in alert.title.lower() or "duration" in alert.message.lower():
            strategies.extend(["optimize_resources", "clear_cache", "restart_workers"])
        
        if "error" in alert.title.lower():
            strategies.extend(["retry_failed_jobs", "reset_environment", "rollback_changes"])
        
        if "health" in alert.title.lower():
            strategies.extend(["restart_services", "clear_temporary_files", "update_dependencies"])
        
        # Resource-specific strategies
        cpu_usage = alert.metrics.get('cpu_usage', 0)
        memory_usage = alert.metrics.get('memory_usage', 0)
        
        if cpu_usage > 0.9:
            strategies.append("scale_cpu_resources")
        
        if memory_usage > 0.85:
            strategies.append("scale_memory_resources")
        
        return strategies
    
    def _execute_repair_strategy(self, alert: GuardAlert, strategy: str) -> bool:
        """Execute a specific repair strategy"""
        
        self.logger.info(f"Executing repair strategy: {strategy} for pipeline {alert.pipeline_id}")
        
        # Simulation of repair strategies
        # In real implementation, these would integrate with CI/CD systems
        
        if strategy == "optimize_resources":
            # Simulate resource optimization
            time.sleep(2)
            return True
        
        elif strategy == "clear_cache":
            # Simulate cache clearing
            time.sleep(1)
            return True
        
        elif strategy == "restart_workers":
            # Simulate worker restart
            time.sleep(3)
            return True
        
        elif strategy == "retry_failed_jobs":
            # Simulate job retry
            time.sleep(2)
            return True
        
        elif strategy == "reset_environment":
            # Simulate environment reset
            time.sleep(5)
            return True
        
        elif strategy == "rollback_changes":
            # Simulate rollback
            time.sleep(4)
            return True
        
        elif strategy == "restart_services":
            # Simulate service restart
            time.sleep(3)
            return True
        
        elif strategy == "scale_cpu_resources":
            # Simulate CPU scaling
            time.sleep(2)
            return True
        
        elif strategy == "scale_memory_resources":
            # Simulate memory scaling
            time.sleep(2)
            return True
        
        else:
            self.logger.warning(f"Unknown repair strategy: {strategy}")
            return False
    
    def _send_notifications(self, alert: GuardAlert):
        """Send alert notifications through configured channels"""
        
        # Webhook notification
        webhook_url = self.config["notifications"].get("webhook_url")
        if webhook_url:
            try:
                # Simulate webhook notification
                self.logger.info(f"Sent webhook notification for alert {alert.id}")
            except Exception as e:
                self.logger.error(f"Failed to send webhook notification: {e}")
        
        # Email notification
        if self.config["notifications"].get("email_enabled"):
            try:
                # Simulate email notification
                self.logger.info(f"Sent email notification for alert {alert.id}")
            except Exception as e:
                self.logger.error(f"Failed to send email notification: {e}")
        
        # Slack notification
        if self.config["notifications"].get("slack_enabled"):
            try:
                # Simulate Slack notification
                self.logger.info(f"Sent Slack notification for alert {alert.id}")
            except Exception as e:
                self.logger.error(f"Failed to send Slack notification: {e}")
    
    def _learn_from_metrics(self, metrics: PipelineMetrics):
        """Learn patterns from pipeline metrics for improved prediction"""
        
        if not metrics.pattern_signature:
            return
        
        # Store successful patterns
        if metrics.get_health_score() > 0.8 and metrics.error_count == 0:
            if metrics.pipeline_id not in self.success_patterns:
                self.success_patterns[metrics.pipeline_id] = []
            
            self.success_patterns[metrics.pipeline_id].append(metrics.pattern_signature)
            
            # Update performance baseline
            current_baseline = self.performance_baselines.get(metrics.pipeline_id, metrics.duration_seconds)
            self.performance_baselines[metrics.pipeline_id] = (current_baseline * 0.9 + metrics.duration_seconds * 0.1)
        
        # Store failure patterns
        elif metrics.get_health_score() < 0.6 or metrics.error_count > 0:
            if metrics.pipeline_id not in self.failure_patterns:
                self.failure_patterns[metrics.pipeline_id] = []
            
            self.failure_patterns[metrics.pipeline_id].append(metrics.pattern_signature)
    
    def _perform_predictive_analysis(self):
        """Perform predictive analysis to prevent future failures"""
        
        if not self.enable_predictive_analysis:
            return
        
        # Analyze patterns for each active pipeline
        for pipeline_id, pipeline_info in self.active_pipelines.items():
            last_metrics = pipeline_info.get('last_metrics')
            if not last_metrics or not last_metrics.pattern_signature:
                continue
            
            # Check similarity to known failure patterns
            failure_risk = self._calculate_failure_risk(pipeline_id, last_metrics.pattern_signature)
            
            if failure_risk > 0.7:  # High failure risk
                # Generate predictive alert
                predictive_alert = GuardAlert(
                    id=str(uuid.uuid4()),
                    timestamp=datetime.now(),
                    severity=AlertSeverity.WARNING,
                    pipeline_id=pipeline_id,
                    phase=last_metrics.phase,
                    title="Predictive Failure Alert",
                    message=f"Pattern analysis indicates {failure_risk:.0%} probability of failure",
                    metrics={'failure_risk': failure_risk},
                    recommended_actions=[
                        "Review recent changes for potential issues",
                        "Consider running additional validation tests",
                        "Monitor pipeline closely for early intervention"
                    ]
                )
                
                self._handle_alert(predictive_alert)
    
    def _calculate_failure_risk(self, pipeline_id: str, current_pattern: HyperVector) -> float:
        """Calculate failure risk based on pattern similarity"""
        
        # Get known failure patterns for this pipeline
        failure_patterns = self.failure_patterns.get(pipeline_id, [])
        success_patterns = self.success_patterns.get(pipeline_id, [])
        
        if not failure_patterns:
            return 0.0
        
        # Calculate similarity to failure patterns
        failure_similarities = [current_pattern.similarity(fp) for fp in failure_patterns[-10:]]  # Last 10 patterns
        max_failure_similarity = max(failure_similarities) if failure_similarities else 0.0
        
        # Calculate similarity to success patterns
        if success_patterns:
            success_similarities = [current_pattern.similarity(sp) for sp in success_patterns[-10:]]
            max_success_similarity = max(success_similarities) if success_similarities else 0.0
        else:
            max_success_similarity = 0.0
        
        # Risk calculation: high similarity to failures, low similarity to success
        if max_success_similarity > 0:
            risk = max_failure_similarity / (max_failure_similarity + max_success_similarity)
        else:
            risk = max_failure_similarity
        
        return min(1.0, max(0.0, risk))
    
    def _update_guard_status(self):
        """Update overall guard status based on pipeline health"""
        
        if not self.active_pipelines:
            self.status = GuardStatus.HEALTHY
            return
        
        # Collect health scores from all pipelines
        health_scores = []
        critical_issues = 0
        warning_issues = 0
        
        for pipeline_info in self.active_pipelines.values():
            last_metrics = pipeline_info.get('last_metrics')
            if last_metrics:
                health_score = last_metrics.get_health_score()
                health_scores.append(health_score)
                
                if health_score < 0.5:
                    critical_issues += 1
                elif health_score < 0.7:
                    warning_issues += 1
        
        if not health_scores:
            self.status = GuardStatus.HEALTHY
            return
        
        # Determine overall status
        avg_health = sum(health_scores) / len(health_scores)
        
        if critical_issues > 0:
            self.status = GuardStatus.CRITICAL
        elif warning_issues > len(health_scores) * 0.5:  # More than 50% have warnings
            self.status = GuardStatus.DEGRADED
        elif avg_health < 0.8:
            self.status = GuardStatus.DEGRADED
        else:
            self.status = GuardStatus.HEALTHY
    
    def _cleanup_old_data(self):
        """Clean up old metrics and alerts"""
        
        current_time = datetime.now()
        
        # Clean up old metrics
        retention_days = self.config["monitoring"]["metrics_retention_days"]
        cutoff_time = current_time - timedelta(days=retention_days)
        
        self.metrics_history = [
            m for m in self.metrics_history 
            if m.timestamp > cutoff_time
        ]
        
        # Clean up old alerts
        alert_retention_days = self.config["monitoring"]["alert_retention_days"]
        alert_cutoff_time = current_time - timedelta(days=alert_retention_days)
        
        self.alert_history = [
            a for a in self.alert_history
            if a.timestamp > alert_cutoff_time
        ]
    
    def register_pipeline(self, pipeline_id: str, pipeline_config: Dict[str, Any]):
        """Register a new pipeline for monitoring"""
        
        self.active_pipelines[pipeline_id] = {
            'id': pipeline_id,
            'config': pipeline_config,
            'registered_at': datetime.now(),
            'last_update': None,
            'last_metrics': None,
            'expected_duration': pipeline_config.get('expected_duration', 600),
            'base_success_rate': pipeline_config.get('success_rate', 0.95)
        }
        
        self.logger.info(f"Pipeline registered: {pipeline_id}")
    
    def unregister_pipeline(self, pipeline_id: str):
        """Unregister a pipeline from monitoring"""
        
        if pipeline_id in self.active_pipelines:
            del self.active_pipelines[pipeline_id]
            self.logger.info(f"Pipeline unregistered: {pipeline_id}")
    
    def get_pipeline_health(self, pipeline_id: str) -> Optional[Dict[str, Any]]:
        """Get current health status for a specific pipeline"""
        
        pipeline_info = self.active_pipelines.get(pipeline_id)
        if not pipeline_info:
            return None
        
        last_metrics = pipeline_info.get('last_metrics')
        if not last_metrics:
            return {'status': 'no_data', 'health_score': 0.0}
        
        health_score = last_metrics.get_health_score()
        
        # Determine status
        if health_score >= 0.8:
            status = "healthy"
        elif health_score >= 0.6:
            status = "degraded"
        else:
            status = "critical"
        
        return {
            'status': status,
            'health_score': health_score,
            'last_update': last_metrics.timestamp,
            'error_count': last_metrics.error_count,
            'duration': last_metrics.duration_seconds,
            'success_rate': last_metrics.success_rate
        }
    
    def get_guard_summary(self) -> Dict[str, Any]:
        """Get comprehensive guard status summary"""
        
        # Recent alerts summary
        recent_alerts = [a for a in self.alert_history if a.timestamp > datetime.now() - timedelta(hours=24)]
        alert_counts = {}
        for severity in AlertSeverity:
            alert_counts[severity.value] = len([a for a in recent_alerts if a.severity == severity])
        
        # Active issues
        active_alerts = [a for a in recent_alerts if not a.resolved]
        
        # Pipeline summary
        pipeline_summary = {}
        for pipeline_id, pipeline_info in self.active_pipelines.items():
            health = self.get_pipeline_health(pipeline_id)
            pipeline_summary[pipeline_id] = health
        
        return {
            'guard_status': self.status.value,
            'timestamp': datetime.now(),
            'monitored_pipelines': len(self.active_pipelines),
            'alert_counts_24h': alert_counts,
            'active_alerts': len(active_alerts),
            'auto_repairs_attempted': len([a for a in recent_alerts if a.auto_repair_attempted]),
            'successful_repairs': len([a for a in recent_alerts if a.resolved and a.auto_repair_attempted]),
            'pipeline_health': pipeline_summary,
            'metrics_collected': len(self.metrics_history),
            'patterns_learned': {
                'success_patterns': sum(len(patterns) for patterns in self.success_patterns.values()),
                'failure_patterns': sum(len(patterns) for patterns in self.failure_patterns.values())
            }
        }

# Example usage and testing
if __name__ == "__main__":
    # Initialize pipeline guard
    guard = PipelineGuard(
        hdc_dimension=10000,
        enable_auto_repair=True,
        enable_predictive_analysis=True
    )
    
    # Register example pipelines
    guard.register_pipeline("web-app-ci", {
        "expected_duration": 480,  # 8 minutes
        "success_rate": 0.94
    })
    
    guard.register_pipeline("api-service-cd", {
        "expected_duration": 720,  # 12 minutes  
        "success_rate": 0.97
    })
    
    # Start monitoring
    guard.start_monitoring()
    
    try:
        print("🛡️ Pipeline Guard running... (Press Ctrl+C to stop)")
        
        # Run for demonstration
        for i in range(12):  # 6 minutes of monitoring
            time.sleep(30)
            
            # Print status every 2 minutes
            if i % 4 == 0:
                summary = guard.get_guard_summary()
                print(f"\n📊 Guard Status: {summary['guard_status']}")
                print(f"   Monitored Pipelines: {summary['monitored_pipelines']}")
                print(f"   Active Alerts: {summary['active_alerts']}")
                print(f"   Auto-repairs: {summary['auto_repairs_attempted']}")
                
                for pipeline_id, health in summary['pipeline_health'].items():
                    if health:
                        print(f"   {pipeline_id}: {health['status']} ({health['health_score']:.2f})")
    
    except KeyboardInterrupt:
        print("\nStopping pipeline guard...")
    
    finally:
        guard.stop_monitoring()
        print("Pipeline guard stopped.")